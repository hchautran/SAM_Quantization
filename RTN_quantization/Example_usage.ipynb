{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3160edab",
   "metadata": {},
   "source": [
    "# SAM Quantization Example Usage\n",
    "\n",
    "This notebook demonstrates how to use the SAM quantization utilities, including SmoothQuant weight comparison and A8W8 quantization.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide covers two main functionalities:\n",
    "1. **Smooth SAM and SAM SmoothQuant Weight Comparison**: Run the Smooth_sam.py to get smoothed checkpoint. Compare original vs smoothed SAM model weights\n",
    "2. **A8W8 Quantization**: Convert Linear layers to 8-bit weight and activation quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5ea20",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries and utilities for SAM model quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44fb7224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import our custom quantization utilities\n",
    "from utils import (\n",
    "    sam_smoothing_test,\n",
    "    replace_linear_with_target_and_quantize,\n",
    "    smooth_sam\n",
    ")\n",
    "from per_tensor_channel_group import W8A8Linear\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get('CUDA_VISIBLE_DEVICES', '2')\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038dabc",
   "metadata": {},
   "source": [
    "## 2. Smooth SAM and SAM Model Weight Comparison with SmoothQuant\n",
    "\n",
    "### Smooth SAM\n",
    "Run the file Smooth_sam.py to get checkpoint files and weights for the Smooth SAM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31094c8",
   "metadata": {},
   "source": [
    "### Weight Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5a0f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for SAM model paths\n",
    "sam_checkpoint = \"/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/sam_hq_vit_l.pth\"  # Original SAM checkpoint\n",
    "smoothed_sam_checkpoint = \"/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/smoothed_vit_l_sam.pth\"  # Pre-smoothed SAM checkpoint\n",
    "model_type = \"vit_l\"  # SAM model type (vit_b, vit_l, vit_h)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf31c677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING SAM SMOOTHING WEIGHT COMPARISON TEST\n",
      "============================================================\n",
      "/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/sam_hq_vit_l.pth\n",
      "<All keys matched successfully>\n",
      "Sam model loaded successfully!\n",
      "<All keys matched successfully>\n",
      "Smoothed SAM model loaded successfully!\n",
      "\n",
      "================================================================================\n",
      "WEIGHT COMPARISON: ORIGINAL vs SMOOTHED\n",
      "================================================================================\n",
      "\n",
      "--- LAYER 0 ---\n",
      "\n",
      "Norm1 weights:\n",
      "Original norm1 weight: tensor([ 0.1036, -0.0059,  0.0095,  0.0155,  0.0205])\n",
      "Smoothed norm1 weight: tensor([ 0.1840, -0.0077,  0.0105,  0.0147,  0.0160])\n",
      "Norm1 weight change: 4.504203\n",
      "\n",
      "Attention QKV weights (first 5 input features, first 5 output features):\n",
      "Original attn.qkv weight: tensor([[ 0.0586,  0.0102, -0.0102, -0.0015,  0.0006],\n",
      "        [-0.0320,  0.0039,  0.0016,  0.0064,  0.0043],\n",
      "        [ 0.0082, -0.0029,  0.0042, -0.0029, -0.0045],\n",
      "        [-0.0119, -0.0104,  0.0114, -0.0025, -0.0044],\n",
      "        [-0.0090,  0.0071, -0.0099, -0.0021, -0.0042]])\n",
      "Smoothed attn.qkv weight: tensor([[ 0.0330,  0.0078, -0.0093, -0.0016,  0.0008],\n",
      "        [-0.0180,  0.0030,  0.0014,  0.0068,  0.0055],\n",
      "        [ 0.0046, -0.0022,  0.0038, -0.0030, -0.0058],\n",
      "        [-0.0067, -0.0080,  0.0104, -0.0027, -0.0057],\n",
      "        [-0.0050,  0.0054, -0.0090, -0.0022, -0.0054]])\n",
      "QKV weight change: 23.157913\n",
      "\n",
      "Norm2 weights:\n",
      "Original norm2 weight: tensor([ 6.1588e-01,  7.3100e-02, -1.1008e-04,  4.5823e-02,  9.4794e-02])\n",
      "Smoothed norm2 weight: tensor([ 4.0994e-01,  1.4603e-01, -8.3686e-05,  8.1672e-02,  1.1497e-01])\n",
      "Norm2 weight change: 12.958961\n",
      "\n",
      "MLP first layer (lin1) weights (first 5x5):\n",
      "Original mlp.lin1 weight: tensor([[-7.2398e-03, -8.1305e-03,  5.5151e-03, -4.1920e-03, -2.1735e-02],\n",
      "        [ 1.9920e-03,  1.7721e-02, -1.5776e-03, -1.2485e-02, -2.1012e-02],\n",
      "        [ 4.2092e-05,  1.6391e-02, -8.9176e-03,  2.5919e-02, -5.8370e-03],\n",
      "        [-1.4397e-02, -2.0846e-02, -8.8014e-04, -2.0343e-02, -2.6629e-02],\n",
      "        [ 1.2463e-05, -1.3324e-02, -1.3199e-03, -9.8099e-03, -1.7173e-02]])\n",
      "Smoothed mlp.lin1 weight: tensor([[-1.0877e-02, -4.0701e-03,  7.2545e-03, -2.3520e-03, -1.7920e-02],\n",
      "        [ 2.9927e-03,  8.8711e-03, -2.0752e-03, -7.0049e-03, -1.7324e-02],\n",
      "        [ 6.3236e-05,  8.2051e-03, -1.1730e-02,  1.4542e-02, -4.8125e-03],\n",
      "        [-2.1629e-02, -1.0435e-02, -1.1577e-03, -1.1413e-02, -2.1955e-02],\n",
      "        [ 1.8723e-05, -6.6697e-03, -1.7362e-03, -5.5039e-03, -1.4159e-02]])\n",
      "MLP weight change: 91.622070\n",
      "\n",
      "--- LAYER 1 ---\n",
      "\n",
      "Norm1 weights:\n",
      "Original norm1 weight: tensor([-1.1883, -0.0217,  1.3556,  0.0293, -0.0355])\n",
      "Smoothed norm1 weight: tensor([-0.3707, -0.0128,  0.3590,  0.0170, -0.0145])\n",
      "Norm1 weight change: 5.489172\n",
      "\n",
      "Attention QKV weights (first 5 input features, first 5 output features):\n",
      "Original attn.qkv weight: tensor([[-0.0092,  0.0031,  0.0026,  0.0027,  0.0024],\n",
      "        [ 0.0099,  0.0038,  0.0003,  0.0032, -0.0016],\n",
      "        [ 0.0033, -0.0007,  0.0052,  0.0034,  0.0034],\n",
      "        [-0.0063, -0.0018, -0.0028,  0.0019,  0.0035],\n",
      "        [-0.0258, -0.0015, -0.0048, -0.0036,  0.0004]])\n",
      "Smoothed attn.qkv weight: tensor([[-0.0295,  0.0052,  0.0099,  0.0046,  0.0059],\n",
      "        [ 0.0319,  0.0065,  0.0013,  0.0055, -0.0040],\n",
      "        [ 0.0106, -0.0012,  0.0197,  0.0058,  0.0084],\n",
      "        [-0.0201, -0.0031, -0.0104,  0.0033,  0.0085],\n",
      "        [-0.0826, -0.0026, -0.0180, -0.0063,  0.0010]])\n",
      "QKV weight change: 44.160061\n",
      "\n",
      "Norm2 weights:\n",
      "Original norm2 weight: tensor([ 1.6431e-02, -1.8986e-02, -9.7641e-05,  2.4338e-02, -1.3783e-02])\n",
      "Smoothed norm2 weight: tensor([ 0.0104, -0.0121, -0.0002,  0.0166, -0.0096])\n",
      "Norm2 weight change: 9.677059\n",
      "\n",
      "MLP first layer (lin1) weights (first 5x5):\n",
      "Original mlp.lin1 weight: tensor([[ 6.6195e-05, -2.9382e-04,  7.3069e-05,  6.8327e-04,  2.3169e-04],\n",
      "        [ 5.0759e-03,  9.3620e-05, -3.9095e-03,  7.4441e-04, -4.2451e-03],\n",
      "        [-4.1795e-05, -1.0625e-03,  1.4033e-04,  7.3449e-04, -2.0046e-04],\n",
      "        [ 1.9357e-04, -8.9032e-05, -6.5732e-04, -8.8918e-04, -1.1638e-03],\n",
      "        [-5.1287e-05, -9.1418e-04,  9.2333e-05,  6.7035e-04, -1.9768e-05]])\n",
      "Smoothed mlp.lin1 weight: tensor([[ 1.0454e-04, -4.5979e-04,  3.0308e-05,  9.9880e-04,  3.3406e-04],\n",
      "        [ 8.0159e-03,  1.4650e-04, -1.6216e-03,  1.0882e-03, -6.1208e-03],\n",
      "        [-6.6002e-05, -1.6626e-03,  5.8205e-05,  1.0737e-03, -2.8903e-04],\n",
      "        [ 3.0568e-04, -1.3932e-04, -2.7264e-04, -1.2998e-03, -1.6780e-03],\n",
      "        [-8.0993e-05, -1.4306e-03,  3.8298e-05,  9.7991e-04, -2.8502e-05]])\n",
      "MLP weight change: 64.309631\n",
      "\n",
      "================================================================================\n",
      "MASK DECODER TRANSFORMER LAYERS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "--- TRANSFORMER LAYER 0 ---\n",
      "\n",
      "Norm2 weights:\n",
      "Original norm2 weight: tensor([2.5462, 0.0681, 0.2980, 0.1393, 0.3101])\n",
      "Smoothed norm2 weight: tensor([2.5462, 0.0681, 0.2980, 0.1393, 0.3101])\n",
      "Norm2 weight change: 0.000000\n",
      "\n",
      "MLP lin1 weights (first 5x5):\n",
      "Original mlp.lin1 weight: tensor([[ 0.0931, -0.0079, -0.0246,  0.0476, -0.0133],\n",
      "        [ 0.0933, -0.0350, -0.0112,  0.0160, -0.0123],\n",
      "        [ 0.0427,  0.0003, -0.0251, -0.0019, -0.0447],\n",
      "        [ 0.0172,  0.0318, -0.0772, -0.0598,  0.0054],\n",
      "        [ 0.0595,  0.0220, -0.0026,  0.0010,  0.0820]])\n",
      "Smoothed mlp.lin1 weight: tensor([[ 0.0931, -0.0079, -0.0246,  0.0476, -0.0133],\n",
      "        [ 0.0933, -0.0350, -0.0112,  0.0160, -0.0123],\n",
      "        [ 0.0427,  0.0003, -0.0251, -0.0019, -0.0447],\n",
      "        [ 0.0172,  0.0318, -0.0772, -0.0598,  0.0054],\n",
      "        [ 0.0595,  0.0220, -0.0026,  0.0010,  0.0820]])\n",
      "MLP lin1 weight change: 0.000000\n",
      "\n",
      "--- TRANSFORMER LAYER 1 ---\n",
      "\n",
      "Norm2 weights:\n",
      "Original norm2 weight: tensor([1.1432, 0.4128, 1.3466, 0.7295, 1.2391])\n",
      "Smoothed norm2 weight: tensor([1.1432, 0.4128, 1.3466, 0.7295, 1.2391])\n",
      "Norm2 weight change: 0.000000\n",
      "\n",
      "MLP lin1 weights (first 5x5):\n",
      "Original mlp.lin1 weight: tensor([[-0.0177, -0.0511, -0.0015,  0.0232,  0.0316],\n",
      "        [ 0.0070,  0.0991,  0.0064,  0.0284,  0.0098],\n",
      "        [-0.0505,  0.0477,  0.0221, -0.0201, -0.0114],\n",
      "        [-0.0630,  0.0841, -0.0388, -0.0014,  0.0336],\n",
      "        [ 0.0213,  0.0163, -0.0057, -0.0424, -0.0057]])\n",
      "Smoothed mlp.lin1 weight: tensor([[-0.0177, -0.0511, -0.0015,  0.0232,  0.0316],\n",
      "        [ 0.0070,  0.0991,  0.0064,  0.0284,  0.0098],\n",
      "        [-0.0505,  0.0477,  0.0221, -0.0201, -0.0114],\n",
      "        [-0.0630,  0.0841, -0.0388, -0.0014,  0.0336],\n",
      "        [ 0.0213,  0.0163, -0.0057, -0.0424, -0.0057]])\n",
      "MLP lin1 weight change: 0.000000\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✓ Weight comparison completed!\n"
     ]
    }
   ],
   "source": [
    "# Run SAM smoothing test to compare weights\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING SAM SMOOTHING WEIGHT COMPARISON TEST\")\n",
    "print(\"=\" * 60)\n",
    "print\n",
    "print\n",
    "sam_smoothing_test(\n",
    "    sam_checkpoint=sam_checkpoint,\n",
    "    smoothed_sam_checkpoint=smoothed_sam_checkpoint, \n",
    "    model_type=model_type,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Weight comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bf909",
   "metadata": {},
   "source": [
    "### Expected Output Explanation:\n",
    "\n",
    "The output shows weight comparisons for the first 2 transformer blocks in the SAM image encoder:\n",
    "\n",
    "- **Norm1/Norm2 weights**: LayerNorm parameters before attention/MLP layers\n",
    "- **QKV weights**: Query, Key, Value projection weights in attention mechanism\n",
    "- **MLP weights**: First linear layer in MLP (Feed-Forward) blocks\n",
    "- **Weight change**: L2 norm showing magnitude of parameter changes after smoothing\n",
    "\n",
    "**Key insight**: SmoothQuant modifies LayerNorm scales and corresponding linear layer weights to balance quantization difficulty between weights and activations.\n",
    "\n",
    "### What the numbers mean:\n",
    "- **Small norm changes (< 0.1)**: Indicates gentle smoothing\n",
    "- **Larger weight changes**: Shows where smoothing had significant impact\n",
    "- **Zero changes**: No smoothing was applied to that layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557fb77",
   "metadata": {},
   "source": [
    "## 3. A8W8 Quantization with replace_linear_with_target_and_quantize\n",
    "\n",
    "### Purpose:\n",
    "The `replace_linear_with_target_and_quantize()` function converts standard PyTorch Linear layers to quantized A8W8 (8-bit weights, 8-bit activations) layers throughout a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20e86c",
   "metadata": {},
   "source": [
    "### 3.1 Simple Example with Dummy Model\n",
    "\n",
    "Let's start with a simple example using a dummy model to understand how the quantization works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29d8e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before quantization:\n",
      "DummyModel(\n",
      "  (emb): Embedding(1000, 128)\n",
      "  (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=128, bias=False)\n",
      "  (lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
      ")\n",
      "\n",
      "Total parameters: 321,792\n",
      "\n",
      "Linear layers before quantization:\n",
      "  linear_1: Linear - torch.Size([256, 128])\n",
      "  linear_2: Linear - torch.Size([128, 256])\n",
      "  lm_head: Linear - torch.Size([1000, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model for demonstration\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(1000, 128)  # Embedding layer\n",
    "        self.linear_1 = nn.Linear(128, 256)  # First linear layer\n",
    "        self.linear_2 = nn.Linear(256, 128, bias=False)  # Second linear layer (no bias)\n",
    "        self.lm_head = nn.Linear(128, 1000, bias=False)  # Output head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "# Create and inspect the model\n",
    "model = DummyModel()\n",
    "print(\"Before quantization:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# List all linear layers\n",
    "print(\"\\nLinear layers before quantization:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"  {name}: {type(module).__name__} - {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3de25da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying A8W8 quantization...\n",
      "\n",
      "After quantization:\n",
      "DummyModel(\n",
      "  (emb): Embedding(1000, 128)\n",
      "  (linear_1): W8A8Linear(128, 256, bias=True, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "  (linear_2): W8A8Linear(256, 128, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "  (lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
      ")\n",
      "\n",
      "Layers after quantization:\n",
      "  linear_1: W8A8Linear (QUANTIZED)\n",
      "  linear_2: W8A8Linear (QUANTIZED)\n",
      "  lm_head: Linear (ORIGINAL)\n"
     ]
    }
   ],
   "source": [
    "# Apply A8W8 quantization to the model\n",
    "# We exclude 'emb' (embedding) and 'lm_head' (output head) from quantization\n",
    "print(\"Applying A8W8 quantization...\")\n",
    "\n",
    "replace_linear_with_target_and_quantize(\n",
    "    module=model, \n",
    "    target_class=W8A8Linear, \n",
    "    module_name_to_exclude=[\"emb\", \"lm_head\"],  # Skip embedding and output head\n",
    "    weight_quant=\"per_channel\",  # Per-channel weight quantization\n",
    "    act_quant=\"per_token\"       # Per-token activation quantization\n",
    ")\n",
    "\n",
    "print(\"\\nAfter quantization:\")\n",
    "print(model)\n",
    "\n",
    "# List all layers after quantization\n",
    "print(\"\\nLayers after quantization:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Linear, W8A8Linear)):\n",
    "        layer_type = \"QUANTIZED\" if isinstance(module, W8A8Linear) else \"ORIGINAL\"\n",
    "        print(f\"  {name}: {type(module).__name__} ({layer_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3544724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing quantized model...\n",
      "Input shape: torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 1000])\n",
      "Output range: [-0.406, 0.390]\n",
      "✓ Forward pass successful with quantized model!\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass to ensure the quantized model works\n",
    "print(\"Testing quantized model...\")\n",
    "\n",
    "# Create a sample input\n",
    "test_input = torch.randint(0, 1000, (2, 10))  # Batch size 2, sequence length 10\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "\n",
    "print(\"✓ Forward pass successful with quantized model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8477605c",
   "metadata": {},
   "source": [
    "### 3.2 Applying A8W8 Quantization to SAM Model\n",
    "\n",
    "Now let's apply quantization to a real SAM model. This is more complex due to SAM's architecture with image encoder, prompt encoder, and mask decoder components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fec2d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM model: vit_l\n",
      "<All keys matched successfully>\n",
      "✓ SAM model loaded successfully!\n",
      "\n",
      "==================================================\n",
      "BEFORE QUANTIZATION\n",
      "==================================================\n",
      "Layer 0 QKV: Linear\n",
      "Layer 0 MLP: Linear\n",
      "Layer 1 QKV: Linear\n",
      "Layer 1 MLP: Linear\n"
     ]
    }
   ],
   "source": [
    "sam_model_checkpoint = \"/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/sam_hq_vit_l.pth\"\n",
    "sam_model_type = \"vit_l\"\n",
    "\n",
    "print(f\"Loading SAM model: {sam_model_type}\")\n",
    "sam_model = sam_model_registry[sam_model_type](checkpoint=sam_model_checkpoint)\n",
    "sam_model.to(device)\n",
    "sam_model.eval()\n",
    "# print out the architecture of model.mask_decoder\n",
    "\n",
    "print(\"✓ SAM model loaded successfully!\")\n",
    "\n",
    "# Show first 2 attention layers before quantization\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEFORE QUANTIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(2):\n",
    "    if hasattr(sam_model.image_encoder, 'blocks') and i < len(sam_model.image_encoder.blocks):\n",
    "        block = sam_model.image_encoder.blocks[i]\n",
    "        \n",
    "        # Check QKV layer\n",
    "        if hasattr(block, 'attn') and hasattr(block.attn, 'qkv'):\n",
    "            qkv_type = type(block.attn.qkv).__name__\n",
    "            print(f\"Layer {i} QKV: {qkv_type}\")\n",
    "        \n",
    "        # Check MLP layer  \n",
    "        if hasattr(block, 'mlp'):\n",
    "            mlp_layer = getattr(block.mlp, 'lin1', getattr(block.mlp, 'fc1', None))\n",
    "            if mlp_layer:\n",
    "                mlp_type = type(mlp_layer).__name__\n",
    "                print(f\"Layer {i} MLP: {mlp_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a0f827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "APPLYING QUANTIZATION\n",
      "==================================================\n",
      "✓ Quantization completed!\n"
     ]
    }
   ],
   "source": [
    "# Apply A8W8 quantization to SAM model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"APPLYING QUANTIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# More targeted exclusion list for SAM-HQ\n",
    "modules_to_exclude = [\n",
    "    \"pos_embed\", \"cls_token\", \"patch_embed\", \n",
    "    \"neck\", \"fpn\", \"mask_tokens\", \"iou_token\", \n",
    "    \"output_upscaling\", \"output_hypernetworks_mlps\"\n",
    "]\n",
    "\n",
    "# Apply quantization\n",
    "replace_linear_with_target_and_quantize(\n",
    "    module=sam_model,\n",
    "    target_class=W8A8Linear,\n",
    "    module_name_to_exclude=modules_to_exclude,\n",
    "    weight_quant=\"per_channel\",    \n",
    "    act_quant=\"per_token\",           \n",
    "    quantize_output=False\n",
    ")\n",
    "\n",
    "print(\"✓ Quantization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90fb8fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "AFTER QUANTIZATION\n",
      "==================================================\n",
      "Quantized layers: 146\n",
      "Original layers: 0\n",
      "Layer 0 QKV: W8A8Linear (✓ Quantized)\n",
      "Layer 0 MLP: W8A8Linear (✓ Quantized)\n",
      "Layer 1 QKV: W8A8Linear (✓ Quantized)\n",
      "Layer 1 MLP: W8A8Linear (✓ Quantized)\n",
      "\n",
      "Quantization ratio: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AFTER QUANTIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count quantized vs original layers\n",
    "quantized_layers = sum(1 for _, m in sam_model.named_modules() if isinstance(m, W8A8Linear))\n",
    "linear_layers = sum(1 for _, m in sam_model.named_modules() if isinstance(m, nn.Linear))\n",
    "\n",
    "print(f\"Quantized layers: {quantized_layers}\")\n",
    "print(f\"Original layers: {linear_layers}\")\n",
    "\n",
    "# Show first 2 attention layers\n",
    "for i in range(2):\n",
    "    if hasattr(sam_model.image_encoder, 'blocks') and i < len(sam_model.image_encoder.blocks):\n",
    "        block = sam_model.image_encoder.blocks[i]\n",
    "        \n",
    "        # Check QKV layer\n",
    "        if hasattr(block, 'attn') and hasattr(block.attn, 'qkv'):\n",
    "            qkv_type = type(block.attn.qkv).__name__\n",
    "            status = \"✓ Quantized\" if isinstance(block.attn.qkv, W8A8Linear) else \"⚠️ Not quantized\"\n",
    "            print(f\"Layer {i} QKV: {qkv_type} ({status})\")\n",
    "        \n",
    "        # Check MLP layer\n",
    "        if hasattr(block, 'mlp'):\n",
    "            mlp_layer = getattr(block.mlp, 'lin1', getattr(block.mlp, 'fc1', None))\n",
    "            if mlp_layer:\n",
    "                mlp_type = type(mlp_layer).__name__\n",
    "                status = \"✓ Quantized\" if isinstance(mlp_layer, W8A8Linear) else \"⚠️ Not quantized\"\n",
    "                print(f\"Layer {i} MLP: {mlp_type} ({status})\")\n",
    "\n",
    "if quantized_layers + linear_layers > 0:\n",
    "    ratio = quantized_layers / (quantized_layers + linear_layers) * 100\n",
    "    print(f\"\\nQuantization ratio: {ratio:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
