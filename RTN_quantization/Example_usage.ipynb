{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3160edab",
   "metadata": {},
   "source": [
    "# SAM Quantization Example Usage\n",
    "\n",
    "This notebook demonstrates how to use the SAM quantization utilities, including SmoothQuant weight comparison and A8W8 quantization.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide covers two main functionalities:\n",
    "1. **Smooth SAM and SAM SmoothQuant Weight Comparison**: Run the Smooth_sam.py to get smoothed checkpoint. Compare original vs smoothed SAM model weights\n",
    "2. **A8W8 Quantization**: Convert Linear layers to 8-bit weight and activation quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5ea20",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries and utilities for SAM model quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fb7224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('/path/to/SAM_Quantization')\n",
    "# Import our custom quantization utilities\n",
    "from utils import (\n",
    "    sam_smoothing_test,\n",
    "    replace_linear_with_target_and_quantize,\n",
    "    smooth_sam\n",
    ")\n",
    "from per_tensor_channel_group import W8A8Linear\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get('CUDA_VISIBLE_DEVICES', '2')\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038dabc",
   "metadata": {},
   "source": [
    "## 2. Smooth SAM and SAM Model Weight Comparison with SmoothQuant\n",
    "\n",
    "### Smooth SAM\n",
    "Run the file Smooth_sam.py to get checkpoint files and weights for the Smooth SAM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31094c8",
   "metadata": {},
   "source": [
    "### Weight Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a0f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for SAM model paths\n",
    "sam_checkpoint = \"/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/sam_hq_vit_l.pth\"  # Original SAM checkpoint\n",
    "smoothed_sam_checkpoint = \"/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/smoothed_vit_l_sam.pth\"  # Pre-smoothed SAM checkpoint\n",
    "model_type = \"vit_l\"  # SAM model type (vit_b, vit_l, vit_h)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf31c677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUNNING SAM SMOOTHING WEIGHT COMPARISON TEST\n",
      "============================================================\n",
      "/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/sam_hq_vit_l.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/sam_hq_vit_l.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43msam_smoothing_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msam_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msam_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmoothed_sam_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmoothed_sam_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Weight comparison completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/caduser/MyBook/chau/chi/SAM_Quantization/RTN_quantization/utils.py:358\u001b[0m, in \u001b[0;36msam_smoothing_test\u001b[0;34m(sam_checkpoint, smoothed_sam_checkpoint, model_type, device)\u001b[0m\n\u001b[1;32m    355\u001b[0m model_type \u001b[38;5;241m=\u001b[39m model_type\n\u001b[1;32m    356\u001b[0m device \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m--> 358\u001b[0m original_sam \u001b[38;5;241m=\u001b[39m \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msam_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m original_sam\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    360\u001b[0m original_sam\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/media/caduser/MyBook/chau/chi/SAM_Quantization/sam-hq/segment_anything/build_sam.py:28\u001b[0m, in \u001b[0;36mbuild_sam_vit_l\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_sam_vit_l\u001b[39m(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m23\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/caduser/MyBook/chau/chi/SAM_Quantization/sam-hq/segment_anything/build_sam.py:160\u001b[0m, in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[0m\n\u001b[1;32m    158\u001b[0m sam\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    161\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/sam_hq_vit_l.pth'"
     ]
    }
   ],
   "source": [
    "# Run SAM smoothing test to compare weights\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING SAM SMOOTHING WEIGHT COMPARISON TEST\")\n",
    "print(\"=\" * 60)\n",
    "print\n",
    "print\n",
    "sam_smoothing_test(\n",
    "    sam_checkpoint=sam_checkpoint,\n",
    "    smoothed_sam_checkpoint=smoothed_sam_checkpoint, \n",
    "    model_type=model_type,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Weight comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978bf909",
   "metadata": {},
   "source": [
    "### Expected Output Explanation:\n",
    "\n",
    "The output shows weight comparisons for the first 2 transformer blocks in the SAM image encoder:\n",
    "\n",
    "- **Norm1/Norm2 weights**: LayerNorm parameters before attention/MLP layers\n",
    "- **QKV weights**: Query, Key, Value projection weights in attention mechanism\n",
    "- **MLP weights**: First linear layer in MLP (Feed-Forward) blocks\n",
    "- **Weight change**: L2 norm showing magnitude of parameter changes after smoothing\n",
    "\n",
    "**Key insight**: SmoothQuant modifies LayerNorm scales and corresponding linear layer weights to balance quantization difficulty between weights and activations.\n",
    "\n",
    "### What the numbers mean:\n",
    "- **Small norm changes (< 0.1)**: Indicates gentle smoothing\n",
    "- **Larger weight changes**: Shows where smoothing had significant impact\n",
    "- **Zero changes**: No smoothing was applied to that layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557fb77",
   "metadata": {},
   "source": [
    "## 3. A8W8 Quantization with replace_linear_with_target_and_quantize\n",
    "\n",
    "### Purpose:\n",
    "The `replace_linear_with_target_and_quantize()` function converts standard PyTorch Linear layers to quantized A8W8 (8-bit weights, 8-bit activations) layers throughout a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20e86c",
   "metadata": {},
   "source": [
    "### 3.1 Simple Example with Dummy Model\n",
    "\n",
    "Let's start with a simple example using a dummy model to understand how the quantization works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before quantization:\n",
      "DummyModel(\n",
      "  (emb): Embedding(1000, 128)\n",
      "  (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=128, bias=False)\n",
      "  (lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
      ")\n",
      "\n",
      "Total parameters: 321,792\n",
      "\n",
      "Linear layers before quantization:\n",
      "  linear_1: Linear - torch.Size([256, 128])\n",
      "  linear_2: Linear - torch.Size([128, 256])\n",
      "  lm_head: Linear - torch.Size([1000, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model for demonstration\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(1000, 128)  # Embedding layer\n",
    "        self.linear_1 = nn.Linear(128, 256)  # First linear layer\n",
    "        self.linear_2 = nn.Linear(256, 128, bias=False)  # Second linear layer (no bias)\n",
    "        self.lm_head = nn.Linear(128, 1000, bias=False)  # Output head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "# Create and inspect the model\n",
    "model = DummyModel()\n",
    "print(\"Before quantization:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# List all linear layers\n",
    "print(\"\\nLinear layers before quantization:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"  {name}: {type(module).__name__} - {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de25da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying A8W8 quantization...\n",
      "\n",
      "After quantization:\n",
      "DummyModel(\n",
      "  (emb): Embedding(1000, 128)\n",
      "  (linear_1): W8A8Linear(128, 256, bias=True, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "  (linear_2): W8A8Linear(256, 128, bias=False, weight_quant=per_channel, act_quant=per_token, output_quant=None)\n",
      "  (lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
      ")\n",
      "\n",
      "Layers after quantization:\n",
      "  linear_1: W8A8Linear (QUANTIZED)\n",
      "  linear_2: W8A8Linear (QUANTIZED)\n",
      "  lm_head: Linear (ORIGINAL)\n"
     ]
    }
   ],
   "source": [
    "# Apply A8W8 quantization to the model\n",
    "# We exclude 'emb' (embedding) and 'lm_head' (output head) from quantization\n",
    "print(\"Applying A8W8 quantization...\")\n",
    "\n",
    "replace_linear_with_target_and_quantize(\n",
    "    module=model, \n",
    "    target_class=W8A8Linear, \n",
    "    module_name_to_exclude=[\"emb\", \"lm_head\"],  # Skip embedding and output head\n",
    "    weight_quant=\"per_channel\",  # Per-channel weight quantization\n",
    "    act_quant=\"per_token\"       # Per-token activation quantization\n",
    ")\n",
    "\n",
    "print(\"\\nAfter quantization:\")\n",
    "print(model)\n",
    "\n",
    "# List all layers after quantization\n",
    "print(\"\\nLayers after quantization:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Linear, W8A8Linear)):\n",
    "        layer_type = \"QUANTIZED\" if isinstance(module, W8A8Linear) else \"ORIGINAL\"\n",
    "        print(f\"  {name}: {type(module).__name__} ({layer_type})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3544724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing quantized model...\n",
      "Input shape: torch.Size([2, 10])\n",
      "Output shape: torch.Size([2, 10, 1000])\n",
      "Output range: [-0.406, 0.390]\n",
      "✓ Forward pass successful with quantized model!\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass to ensure the quantized model works\n",
    "print(\"Testing quantized model...\")\n",
    "\n",
    "# Create a sample input\n",
    "test_input = torch.randint(0, 1000, (2, 10))  # Batch size 2, sequence length 10\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "\n",
    "print(\"✓ Forward pass successful with quantized model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8477605c",
   "metadata": {},
   "source": [
    "### 3.2 Applying A8W8 Quantization to SAM Model\n",
    "\n",
    "Now let's apply quantization to a real SAM model. This is more complex due to SAM's architecture with image encoder, prompt encoder, and mask decoder components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM model: vit_l\n",
      "<All keys matched successfully>\n",
      "✓ SAM model loaded successfully!\n",
      "\n",
      "==================================================\n",
      "BEFORE QUANTIZATION\n",
      "==================================================\n",
      "Layer 0 QKV: Linear\n",
      "Layer 0 MLP: Linear\n",
      "Layer 1 QKV: Linear\n",
      "Layer 1 MLP: Linear\n"
     ]
    }
   ],
   "source": [
    "sam_model_checkpoint = \"/home/ubuntu/21chi.nh/Quantization/SAM_Quantization/SAM_Quantization/checkpoint_sam/sam_hq_vit_l.pth\"\n",
    "sam_model_type = \"vit_l\"\n",
    "\n",
    "print(f\"Loading SAM model: {sam_model_type}\")\n",
    "sam_model = sam_model_registry[sam_model_type](checkpoint=sam_model_checkpoint)\n",
    "sam_model.to(device)\n",
    "sam_model.eval()\n",
    "# print out the architecture of model.mask_decoder\n",
    "\n",
    "print(\"✓ SAM model loaded successfully!\")\n",
    "\n",
    "# Show first 2 attention layers before quantization\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEFORE QUANTIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(2):\n",
    "    if hasattr(sam_model.image_encoder, 'blocks') and i < len(sam_model.image_encoder.blocks):\n",
    "        block = sam_model.image_encoder.blocks[i]\n",
    "        \n",
    "        # Check QKV layer\n",
    "        if hasattr(block, 'attn') and hasattr(block.attn, 'qkv'):\n",
    "            qkv_type = type(block.attn.qkv).__name__\n",
    "            print(f\"Layer {i} QKV: {qkv_type}\")\n",
    "        \n",
    "        # Check MLP layer  \n",
    "        if hasattr(block, 'mlp'):\n",
    "            mlp_layer = getattr(block.mlp, 'lin1', getattr(block.mlp, 'fc1', None))\n",
    "            if mlp_layer:\n",
    "                mlp_type = type(mlp_layer).__name__\n",
    "                print(f\"Layer {i} MLP: {mlp_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "APPLYING QUANTIZATION\n",
      "==================================================\n",
      "✓ Quantization completed!\n"
     ]
    }
   ],
   "source": [
    "# Apply A8W8 quantization to SAM model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"APPLYING QUANTIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# More targeted exclusion list for SAM-HQ\n",
    "modules_to_exclude = [\n",
    "    \"pos_embed\", \"cls_token\", \"patch_embed\", \n",
    "    \"neck\", \"fpn\", \"mask_tokens\", \"iou_token\", \n",
    "    \"output_upscaling\", \"output_hypernetworks_mlps\"\n",
    "]\n",
    "\n",
    "# Apply quantization\n",
    "replace_linear_with_target_and_quantize(\n",
    "    module=sam_model,\n",
    "    target_class=W8A8Linear,\n",
    "    module_name_to_exclude=modules_to_exclude,\n",
    "    weight_quant=\"per_channel\",    \n",
    "    act_quant=\"per_token\",           \n",
    "    quantize_output=False\n",
    ")\n",
    "\n",
    "print(\"✓ Quantization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb8fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "AFTER QUANTIZATION\n",
      "==================================================\n",
      "Quantized layers: 146\n",
      "Original layers: 0\n",
      "Layer 0 QKV: W8A8Linear (✓ Quantized)\n",
      "Layer 0 MLP: W8A8Linear (✓ Quantized)\n",
      "Layer 1 QKV: W8A8Linear (✓ Quantized)\n",
      "Layer 1 MLP: W8A8Linear (✓ Quantized)\n",
      "\n",
      "Quantization ratio: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AFTER QUANTIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count quantized vs original layers\n",
    "quantized_layers = sum(1 for _, m in sam_model.named_modules() if isinstance(m, W8A8Linear))\n",
    "linear_layers = sum(1 for _, m in sam_model.named_modules() if isinstance(m, nn.Linear))\n",
    "\n",
    "print(f\"Quantized layers: {quantized_layers}\")\n",
    "print(f\"Original layers: {linear_layers}\")\n",
    "\n",
    "# Show first 2 attention layers\n",
    "for i in range(2):\n",
    "    if hasattr(sam_model.image_encoder, 'blocks') and i < len(sam_model.image_encoder.blocks):\n",
    "        block = sam_model.image_encoder.blocks[i]\n",
    "        \n",
    "        # Check QKV layer\n",
    "        if hasattr(block, 'attn') and hasattr(block.attn, 'qkv'):\n",
    "            qkv_type = type(block.attn.qkv).__name__\n",
    "            status = \"✓ Quantized\" if isinstance(block.attn.qkv, W8A8Linear) else \"⚠️ Not quantized\"\n",
    "            print(f\"Layer {i} QKV: {qkv_type} ({status})\")\n",
    "        \n",
    "        # Check MLP layer\n",
    "        if hasattr(block, 'mlp'):\n",
    "            mlp_layer = getattr(block.mlp, 'lin1', getattr(block.mlp, 'fc1', None))\n",
    "            if mlp_layer:\n",
    "                mlp_type = type(mlp_layer).__name__\n",
    "                status = \"✓ Quantized\" if isinstance(mlp_layer, W8A8Linear) else \"⚠️ Not quantized\"\n",
    "                print(f\"Layer {i} MLP: {mlp_type} ({status})\")\n",
    "\n",
    "if quantized_layers + linear_layers > 0:\n",
    "    ratio = quantized_layers / (quantized_layers + linear_layers) * 100\n",
    "    print(f\"\\nQuantization ratio: {ratio:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sonata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
